{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"declare-lab/flan-alpaca-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"declare-lab/flan-alpaca-base\")\n",
    "\n",
    "def alpaca_embeddings(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate dummy decoder input ids\n",
    "    dummy_decoder_input_ids = torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "    # Get the embeddings from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            decoder_input_ids=dummy_decoder_input_ids,\n",
    "        )\n",
    "\n",
    "    embeddings = outputs.encoder_last_hidden_state\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\")\n",
    "\n",
    "def t5_embeddings(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate dummy decoder input ids\n",
    "    dummy_decoder_input_ids = torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "    # Get the embeddings from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            decoder_input_ids=dummy_decoder_input_ids,\n",
    "        )\n",
    "\n",
    "    embeddings = outputs.encoder_last_hidden_state\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t5_embeddings(\"Ich bin ein Berliner\")[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(sentence):\n",
    "    prompt = \"Represent the theme and the authors opinion in the document for clustering: '{}'\".format(sentence)\n",
    "    input_ids = torch.tensor(tokenizer.encode(prompt, return_tensors='pt'))\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.to('cuda')\n",
    "        model.to('cuda')\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model.generate(input_ids)[0]\n",
    "    last_hidden_states = last_hidden_states.cpu().numpy()\n",
    "    return last_hidden_states"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
